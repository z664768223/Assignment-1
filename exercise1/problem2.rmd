---
title: "Question 2"
output: md_document
---
# K-nearest neighbors
Firstly, we only need the data of trim, price, and mileage for this question. So we clean the other unrelated variable in the data set.
```{r include=FALSE}
library(mosaic)
library(tidyverse)
library(FNN)
sclass <- read.csv("C:/Users/Lydia Liu/Desktop/data mining/sclass.csv.txt")
```
```{r}
sclass<- sclass %>%
  select(trim, mileage, price) 
summary(sclass)
```
Then, we select two specific trim levels from the full data set: 350 and 65 AMG. 
```{r}
sclass350 = subset(sclass, trim == '350')
sclass65AMG = subset(sclass, trim == '65 AMG')
```


**FOR 350 TRIM LEVEL DATA SET**

Take a look at price vs mileage for 350 trim level.
```{r}
plot(price ~ mileage, data = sclass350)
```

We split the data into a training and a testing set and separate the training and testing sets into features (X) and outcome (y).
```{r}
# Make a train-test split
N = nrow(sclass350)
N_train = floor(0.8*N)
N_test = N - N_train

# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)

# Define the training and testing set
D_train = sclass350[train_ind,]
D_test = sclass350[-train_ind,]

# reorder the rows of the testing set by the mileage variable
D_test = arrange(D_test, mileage)

X_train = select(D_train, mileage)
y_train = select(D_train, price)
X_test = select(D_test, mileage)
y_test = select(D_test, price)
```
```{r include=FALSE}
rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}
```
Beacause we only have 416 data points in the 350 trim level sub-dataset. We assume optimum K value for the smallest RMSE in the testing set is less than 100. Set up a for loop to calculate the RMSE for K from 3 to 100 and save the RMSE values in a vector.
```{r}
K = seq(3, 100, by = 1)
u <- 0
for (i in 1:98){
  
  knn = knn.reg(train = X_train, test = X_test, y = y_train, k=K[i])
  ypred_knn = knn$pred
  u[i] = rmse(y_test, ypred_knn)

}
```
Make a plot of RMSE versus K and then find the K value with the minimum RMSE in the graph.
```{r}
ggplot(data = NULL, aes(x = K, y = u)) + 
  geom_path(aes(x = K, y = u),color = 'red') + 
  labs(title = "RMSE versus K", y = "RMSE", x = "K")
```

The optimal value of K for 350 trim level is `r K[which.min(u)]`.
Lastly, we use the optimum K value and present a plot of the fitted model for 350 trim level.
```{r}
opt350 <- K[which.min(u)]
knn = knn.reg(train = X_train, test = X_test, y = y_train, k=opt350)
ypred_knn = knn$pred
D_test$ypred_knn = ypred_knn
ggplot(data = D_test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + 
  ylim(6600, 150000) + geom_path(aes(x = mileage, y = ypred_knn), color='red')
```

**FOR 65 AMG TRIM LEVEL DATA SET**

Take a look at price vs mileage for 65 AMG trim level.
```{r}
plot(price ~ mileage, data = sclass65AMG)
```

Basiclly, we just repeat what we did above. Since we only have 292 samples in this data set. We assume the optimum K value is less than 50.
```{r include=FALSE}
N = nrow(sclass65AMG)
N_train = floor(0.8*N)
N_test = N - N_train
#####
# Train/test split
#####

# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)

# Define the training and testing set
D_train = sclass65AMG[train_ind,]
D_test = sclass65AMG[-train_ind,]

# optional book-keeping step:
# reorder the rows of the testing set by the KHOU (temperature) variable
# this isn't necessary, but it will allow us to make a pretty plot later
D_test = arrange(D_test, mileage)
head(D_test)

# Now separate the training and testing sets into features (X) and outcome (y)
X_train = select(D_train, mileage)
y_train = select(D_train, price)
X_test = select(D_test, mileage)
y_test = select(D_test, price)

# define a helper function for calculating RMSE
rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}
```
```{r}
K = seq(3, 50, by = 1)
u <- 0
for (i in 1:48){
  
  knn = knn.reg(train = X_train, test = X_test, y = y_train, k=K[i])
  ypred_knn = knn$pred
  u[i] = rmse(y_test, ypred_knn)
  
}

ggplot(data = NULL, aes(x = K, y = u)) + 
  geom_path(aes(x = K, y = u),color = 'red') + 
  labs(title = "RMSE versus K", y = "RMSE", x = "K")
```

The optimal value of K for 65 AMG trim level is `r K[which.min(u)]`.
Lastly, we use the optimum K value and present a plot of the fitted model for 65 AMG trim level.
```{r}
opt65 <- K[which.min(u)]
knn = knn.reg(train = X_train, test = X_test, y = y_train, k=opt65)
ypred_knn = knn$pred
D_test$ypred_knn = ypred_knn
ggplot(data = D_test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + 
  ylim(15000, 250000) + geom_path(aes(x = mileage, y = ypred_knn), color='red')

```

Comparing our results, we believe that the reason why 350 trim level data set have a larger optimum K value is because we have more data for 350 trim level cars. So with a larger data set, generally we should have a larger optimum K value for 350 trim level. However, it can also happen that 65 AMG trim level has a larger optimum K value. Because we select random data points for training data set and testing data set, everytime we run the code again we will have a different result and a different graph of RMSE versus K value. So it will also exist the case that we have a larger optimum K value for 65 AMG trim level.