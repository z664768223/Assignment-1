---
title: "Problem 3 news"
author: "Xiaoyu Liu, Yihao Zhang"
date: "3/5/2020"
output:
  html_document: default
  pdf_document: default
---

The following report is going to discuss how to predict whether an article published by Mashable is going to be viral based on Information of 39,797 online articles published by Mashable during 2013 and 2014. and I used two approaches to make predictions.
First approach: build the model to predict the number of shares first and then add the thresholds.
Second approach: create a variable called viral based on the thresholds and then build the model to predict the probability for an article to be viral.

Two thresholds,
1. if shares exceeds 1400, predict the article as "viral"
2. if shares are 1400 or lower, predict the article as "not viral"

```{r setup, include=FALSE}
library(tidyverse)
library(mosaic)
library(FNN)
library(foreach)
library(gamlr) 
library(fpp2)
online_news<-read.csv("online_news.csv")

```

First approach
Since there are more than 30 variables, which is a large set of potential explanatory variables, we decided to use the LASSO regression method to predict the number of shares. This method can drive the coefficients of irrelevant variables to zero, thus performing an automatic variable selection. 

```{r,echo =FALSE}
News <- online_news[-1] %>%
  select(shares, everything())
News <- na.omit(News) %>%
  mutate(shares=log(shares))


# Create the grid of values for lambda, the tuning parameter 
x <- sparse.model.matrix(shares ~ ., News)[,-1]
y <- News$shares
lasso1 = gamlr(x, y, alpha = 1)
plot(lasso1,sub = "for first approach")
```

The above plot represents the coefficients of all variables regarding a range of $log(\lambda)$ values from -7 to -2. 
To find the best value of $\lambda$ to built the model, first, we computed the AICc for each value of $log(\lambda)$ in the above range. 

```{r,echo =FALSE}
plot(lasso1$lambda, AICc(lasso1))
plot(log(lasso1$lambda), AICc(lasso1))
```

But based on the plot of AICc regarding both $log(\lambda)$ and $\lambda$, there doesn't exist an optimal value for $\lambda$ since the AICc only decreases when $\lambda$ decreases. So, we used cross validation with  10-fold to find the optimal $\lambda$ for our lasso regression model.

```{r, echo = FALSE}
# use cross validation
# 10 fold
set.seed(1)
K = 10 
# cross validated lasso (verb just prints progress)
cvl1 = cv.gamlr(x, y, nfold=K)

# plot the out-of-sample deviance as a function of log lambda
plot(cvl1, bty="n")

# CV min deviance selection
log(cvl1$lambda.1se)
```

With cross validation, we take $log(\lambda)=-3.911$  as our optimal value for the LASSO model since it is the largest $log(\lambda)$ value that lies within one standard deviation of the minimum $log(\lambda)$. $log(\lambda)=-3.911$ will turn the LASSO model to include the least amounts of variables, and meanwhile, it has an equivalent significant mean squared error to the minimum $log(\lambda)$. The following tells the coefficients for all significant variables.

```{r,echo = FALSE}
# CV min deviance selection
coef(cvl1, select="1se")
```

During the 10-fold cross validation process, each fold is treated as the testing set. So, there were 10 randomly split training and testing sets. We summarized the 10 confusion matrixes together as well as the overall average overall error rate, true-positive rate, and false-positive rate.

```{r,echo = FALSE}
N = nrow(News)
fold_id = rep_len(1:K, N)
fold_id = sample(fold_id, replace = FALSE)
# error rate
error_rate = rep(0,K)
# True positive rate
tpr = rep(0,K)
# False positive rate
fpr = rep(0,K)

test_set <- News[fold_id == 1,]
xtest <- sparse.model.matrix(shares ~ .,test_set)[,-1]
ytest <- test_set$shares
logyhat1_test = predict(cvl1, xtest, select = "1se")
yhat1_test = exp(logyhat1_test)
result1_test = ifelse(yhat1_test > 1400 , 1, 0)
test_set <- test_set %>%
  mutate(viral = ifelse(exp(test_set$shares) > 1400, 1, 0))
confusion1 = table(y = (test_set$viral), yhat = result1_test)
error_rate[1] = sum(test_set$viral!=result1_test)/nrow(test_set)

tpr[1] = sum((test_set$viral==1 & result1_test==1))/
  sum((test_set$viral==1 & result1_test==1)|(test_set$viral==1 & result1_test==0))
  

fpr[1] = sum((test_set$viral==0 & result1_test==1))/
  sum((test_set$viral==0 & result1_test==1)|(test_set$viral==0 & result1_test==0))
# the rest folder 
for(i in 2:K){
  test_set <- News[fold_id == i,]
  xtest <- sparse.model.matrix(shares ~ .,test_set)[,-1]
  ytest <- test_set$shares
  logyhat1_test = predict(cvl1, xtest, select = "1se")
  yhat1_test = exp(logyhat1_test)
  result1_test = ifelse(yhat1_test > 1400 , 1, 0)
  test_set <- test_set %>%
    mutate(viral = ifelse(exp(test_set$shares) > 1400, 1, 0))
  confusion = table(y = (test_set$viral), yhat = result1_test)
  confusion1 = confusion + confusion1
  error_rate[i] = sum(test_set$viral!=result1_test)/nrow(test_set)
  
  tpr[i] = sum((test_set$viral==1 & result1_test==1))/
  sum((test_set$viral==1 & result1_test==1)|(test_set$viral==1 & result1_test==0))
  
  fpr[i] = sum((test_set$viral==0 & result1_test==1))/
  sum((test_set$viral==0 & result1_test==1)|(test_set$viral==0 & result1_test==0))
}

confusion1

mean(error_rate)

mean(tpr)

mean(fpr)
```

Average Error Rate, 43.76%

Average True-Positive Rate,  90.01%

Average False-Positive Rate, 76.65%

To validate our first model's predicting power, we compared it to the Null model, which always predicts "not viral" for any articles.

```{r,echo = FALSE}
Newsnull <- News %>%
  mutate(predict = 0) %>%
  mutate(viral = ifelse(exp(News$shares) > 1400 , 1, 0))

confusionnull = table(y = (Newsnull$viral), yhat = (Newsnull$predict))
sum(Newsnull$predict!=Newsnull$viral)/39644
```

Our Null model has an error rate of 49.3%, and if we compare this rate to our first predicting model, our model improved its predictive power.
Now, we move to our second approach.

Second approach
Following the method that we used for our first model but now instead, we create a variable called viral first and then built a logistic lasso regression model to predict the probability for any article to be viral. Meanwhile, set up our benchmark at 50.77% for predicting which articles go viral since, among the data, 50.77% of articles are viral. If our second model predicts an article to have more than 50.77% chance to be viral, we consider this article to be viral.

```{r, echo = FALSE}
News2 <- online_news %>%
  select(-url) %>%
  select(shares, everything())

News2 <- na.omit(News2)

News2 <- News2 %>%
  mutate(viral = ifelse(News2$shares > 1400, 1, 0)) %>%
  select(-shares) %>%
  select(viral, everything())
scx <- sparse.model.matrix( viral ~., data=News2)[,-1]
scy <- News2$viral

# fit a single logistic lasso regression model
sclasso = gamlr(scx, scy, alpha = 1,family = "binomial")
# the path plot!

# AIC selected coef
# note: AICc = AIC with small-sample correction.  See ?AICc
plot(sclasso$lambda, AICc(sclasso))
plot(log(sclasso$lambda), AICc(sclasso))
```

Again, the AICc plots based on $\lambda$ tell that we still need to do cross validation to find our model. We generated our 10-Fold cross validation to find our best predicting model.

```{r, echo = FALSE}
set.seed(1)
sccvl = cv.gamlr(scx, scy, nfold=10, verb=TRUE)

# plot the out-of-sample deviance as a function of log lambda
# Q: what are the bars associated with each dot? 
plot(sccvl, bty="n")

## CV min deviance selection
coef(sccvl, select="1se")

## CV 1se selection (the default)
scb.1se = coef(sccvl)
log(sccvl$lambda.1se)
sum(scb.1se!=0)
```

After we found our best model, we summarized the 10 confusion matrixes together as well as the overall average overall error rate, true-positive rate and false-positive rate again.


```{r, echo = FALSE}
set.seed(1)
N = nrow(News2)
fold_id2 = rep_len(1:K, N)
fold_id2 = sample(fold_id, replace = FALSE)
# error rate
error_rate2 = rep(0,K)
# True positive rate
tpr2 = rep(0,K)
# False positive rate
fpr2 = rep(0,K)

test_set2 <- News2[fold_id == 1,]
xtest2 <- sparse.model.matrix(viral ~ .,test_set2)[,-1]
ytest2 <- test_set$viral
yhat2_test = predict(sccvl, xtest2, select = "1se")

result2_test = ifelse(yhat2_test > 0.5077, 1, 0)

confusion2 = table(y = (test_set2$viral), yhat = result2_test)
error_rate2[1] = sum(test_set2$viral!=result2_test)/sum(test_set2)


tpr2[1] = sum((test_set2$viral==1 & result2_test==1))/
  sum((test_set2$viral==1 & result2_test==1)|(test_set2$viral==1 & result2_test==0))


fpr2[1] = sum((test_set2$viral==0 & result2_test==1))/
  sum((test_set2$viral==0 & result2_test==1)|(test_set2$viral==0 & result2_test==0))



# the rest folder

for(i in 2:K){
  test_set2 <- News2[fold_id == i,]
  xtest2 <- sparse.model.matrix(viral ~ .,test_set2)[,-1]
  ytest2 <- test_set2$viral
  yhat2_test = predict(sccvl, xtest2, select = "1se")

  result2_test = ifelse(yhat2_test > 0.5077, 1, 0)
  table(y = (test_set2$viral), yhat = result2_test)
  confusion =  table(y = (test_set2$viral), yhat = result2_test)
  confusion2 = confusion + confusion2
  error_rate2[i] = sum(test_set2$viral!=result2_test)/nrow(test_set2)
  
  tpr2[i] = sum((test_set2$viral==1 & result2_test==1))/
  sum((test_set2$viral==1 & result2_test==1)|(test_set2$viral==1 & result2_test==0))
  
  fpr2[i] =  sum((test_set2$viral==0 & result2_test==1))/
  sum((test_set2$viral==0 & result2_test==1)|(test_set2$viral==0 & result2_test==0))
}

confusion2
mean(error_rate2)
mean(tpr2)
mean(fpr2)
```

Average Error Rate,33.30%

Average True-Positive Rate, 63.39%

Average False-Positive Rate, 37.38%

In the end, we compared our two models and suggested that the second model that added thresholds first have a more reliable predicting power. It has a lower error rate and a flatter true-positive and false-positive rate.  

The reason we think that why threshold first is better is because when we use the regress to predict the shares, some factors may have no linear relation with shares. The residuals may be more significant than the residuals when prediction with binary variables. We think the variables in the data are not good at creating a low variance linear relationship to predict the numerical amount of shares. The variance in predicting the numerical number of shares are eliminated in the binary variables prediction.